# Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods

The repository contains the code for the `SEPO` algorithm presented in the paper:

*[Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods](https://arxiv.org/abs/2502.01384)*.

`SEPO` is an efficient, broadly applicable, and theoretically justified policy gradient algorithm, for fine-tuning discrete diffusion models over general rewards.

**The code will be uploaded mid-February 2025...**

In the mean time, enjoy this pretty GIF of a denoising diffusion process guided by `SEPO`, in the discrete case of language!

<p align="center">
<img src="https://github.com/ozekri/SEPO/blob/main/img/denoising_RLHF.gif" width=50% height=50% alt>
</p>
